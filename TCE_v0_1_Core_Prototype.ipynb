{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1EDat_7Br8TyUGp96USzO-Y-WZCwTHTWN",
      "authorship_tag": "ABX9TyOwdKJyp35b4pQYeyqC6BCn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beachcities/TCE/blob/main/TCE_v0_1_Core_Prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ TCE v0.1 Final Report: ä½æ‰€åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã®æ¤œè¨¼çµæžœ\n",
        "\n",
        "## 1. æ¤œè¨¼ã®èƒŒæ™¯ã¨èª²é¡Œ\n",
        "æ—¥æœ¬ã®ä½æ‰€ãƒ‡ãƒ¼ã‚¿ï¼ˆä½ç½®å‚ç…§æƒ…å ±ï¼‰ã¨è¡Œæ”¿ç•Œãƒãƒªã‚´ãƒ³ï¼ˆç”ºä¸ç›®å¢ƒç•Œï¼‰ã‚’ç”¨ã„ãŸä½æ‰€åˆ¤å®šã‚¨ãƒ³ã‚¸ãƒ³ã€ŒTCEã€ã®ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—æ¤œè¨¼ã‚’è¡Œã£ãŸã€‚\n",
        "å½“åˆã®å˜ç´”ãªã€ŒåŒ…å«åˆ¤å®šï¼ˆPoint in Polygonï¼‰ã€ã§ã¯ç²¾åº¦ **99.31%** ã‚’è¨˜éŒ²ã—ãŸãŒã€ä»¥ä¸‹ã®èª²é¡ŒãŒç‰¹å®šã•ã‚ŒãŸã€‚\n",
        "* **é“è·¯ãƒ»æ²³å·ã®ç©ºç™½å•é¡Œ:** e-Statãƒãƒªã‚´ãƒ³ã¯è¡—åŒºã®ã¿ã‚’å®šç¾©ã—ã¦ãŠã‚Šã€é“è·¯ã‚„æ²³å·ãŒå«ã¾ã‚Œãªã„ãŸã‚ã€é“è·¯ä¸Šã®åº§æ¨™ãŒã€Œå£ã®å¤–ï¼ˆOut of Boundsï¼‰ã€ã¨åˆ¤å®šã•ã‚Œã‚‹ã€‚\n",
        "\n",
        "## 2. ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ¯”è¼ƒæ¤œè¨¼\n",
        "èª²é¡Œè§£æ±ºã®ãŸã‚ã€2ã¤ã®å¹¾ä½•å­¦çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¯”è¼ƒæ¤œè¨¼ã—ãŸã€‚\n",
        "\n",
        "| ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ | æ‰‹æ³• | çµæžœ | è©•ä¾¡ |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **A. ãƒãƒƒãƒ•ã‚¡æ³•** | å£ã‚’ä¸€å¾‹ã«å¤ªã‚‰ã›ã¦éš™é–“ã‚’åŸ‹ã‚ã‚‹ | **ä¸æŽ¡ç”¨** | è·¯åœ°ã®éš™é–“ã¯åŸ‹ã¾ã‚‹ãŒã€å¤§é€šã‚Šï¼ˆå¹…å“¡20mä»¥ä¸Šï¼‰ã®ä¸­å¿ƒã¾ã§å±Šã‹ãšã€‚é€†ã«åºƒã’ã™ãŽã‚‹ã¨éš£æŽ¥è¡—åŒºã¸èª¤åˆ¤å®šã‚’èµ·ã“ã™ãŸã‚ã€ä¸€å¾‹å‡¦ç†ã«é™ç•Œã‚ã‚Šã€‚ |\n",
        "| **B. ãƒœãƒ­ãƒŽã‚¤æ³•** | æœ€ã‚‚è¿‘ã„å£ã‚’æŽ¢ã™ï¼ˆæœ€è¿‘å‚åˆ¤å®šï¼‰ | **æŽ¡ç”¨ (99.43%)** | é“è·¯å¹…ã«é–¢ä¿‚ãªãã€å¹¾ä½•å­¦çš„ã«ã€Œä¸­å¿ƒç·šã€ã§å¢ƒç•Œã‚’å¼•ãã®ã¨åŒç­‰ã®åŠ¹æžœãŒå¾—ã‚‰ã‚ŒãŸã€‚ |\n",
        "\n",
        "## 3. æœ€çµ‚çµè«– (Conclusion)\n",
        "**ã€Œæœ€è¿‘å‚åˆ¤å®šï¼ˆNearest Neighbor Logicï¼‰ã€** ã‚’æŽ¡ç”¨ã™ã‚‹ã“ã¨ã§ã€é“è·¯ãƒ»æ²³å·ä¸Šã®ç‚¹ã‚’å«ã‚€ã™ã¹ã¦ã®åº§æ¨™ã‚’æ•‘æ¸ˆã™ã‚‹ã“ã¨ã«æˆåŠŸã—ãŸã€‚\n",
        "\n",
        "* **æœ€çµ‚ç²¾åº¦:** **99.4295%**\n",
        "* **æ®‹å­˜ã‚¨ãƒ©ãƒ¼:** 10ä»¶ (0.57%)\n",
        "    * ã“ã‚Œã‚‰ã¯å…¨ã¦ã€Œéš£æŽ¥ã™ã‚‹ç”ºä¸ç›®é–“ã§ã®å¢ƒç•Œç·šä¸Šã®å¾®ç´°ãªã‚ºãƒ¬ï¼ˆBorder Jitterï¼‰ã€ã§ã‚ã‚Šã€å»ºç‰©ã®å…¥ã‚Šå£ä½ç½®ã¨é‡å¿ƒä½ç½®ã®ã‚ºãƒ¬ã‚„ã€æ¸¬é‡æ™‚æœŸã®é•ã„ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ã®èª¤å·®ã§ã‚ã‚‹ã€‚\n",
        "* **åˆ¤å®š:** TCE v0.1ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯å®Ÿç”¨ä¸Šã®ç†è«–å€¤ï¼ˆTheoretical Limitï¼‰ã«åˆ°é”ã—ãŸã¨åˆ¤æ–­ã—ã€æœ¬æ¤œè¨¼ã‚’å®Œäº†ã¨ã™ã‚‹ã€‚"
      ],
      "metadata": {
        "id": "4N-vAMPWBb64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === CELL 4: Buffer Optimizer (Metric Experiment) ===\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import zipfile\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "BASE_PATH = \"/content/drive/MyDrive/0_Professional/TCE_Project/data\"\n",
        "\n",
        "def run_optimizer_metric():\n",
        "    print(\"ðŸ“Š Starting Buffer Optimization (Metric System Check)...\")\n",
        "    if not os.path.exists(BASE_PATH): return\n",
        "\n",
        "    # --- ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹å®š ---\n",
        "    files = os.listdir(BASE_PATH)\n",
        "    shp_info = ref_info = None\n",
        "    for f in files:\n",
        "        if not f.lower().endswith(\".zip\"): continue\n",
        "        full_path = os.path.join(BASE_PATH, f)\n",
        "        try:\n",
        "            with zipfile.ZipFile(full_path, 'r') as z:\n",
        "                has_shp = any(n.lower().endswith('.shp') for n in z.namelist())\n",
        "                csv_files = [n for n in z.namelist() if n.lower().endswith('.csv') and not n.startswith('__')]\n",
        "                if has_shp and (shp_info is None) and (\"13101\" in f or \"r2ka\" in f or \"A002\" in f): shp_info = full_path\n",
        "                if len(csv_files) > 0 and (ref_info is None): ref_info = (full_path, csv_files[0])\n",
        "        except: continue\n",
        "\n",
        "    if not shp_info or not ref_info: return\n",
        "\n",
        "    # --- ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ ---\n",
        "    gdf_bound = gpd.read_file(f\"zip://{shp_info}\", encoding='cp932', engine='pyogrio')\n",
        "    gdf_bound = gdf_bound[['KEY_CODE', 'S_NAME', 'geometry']]\n",
        "    with zipfile.ZipFile(ref_info[0], 'r') as z:\n",
        "        with z.open(ref_info[1]) as f: df_ref = pd.read_csv(f, encoding='cp932')\n",
        "    gdf_ref = gpd.GeoDataFrame(df_ref, geometry=gpd.points_from_xy(df_ref['çµŒåº¦'], df_ref['ç·¯åº¦']), crs=\"EPSG:6668\")\n",
        "\n",
        "    # --- ãƒ¡ãƒ¼ãƒˆãƒ«å˜ä½å¤‰æ› (EPSG:6677) ---\n",
        "    print(\"ðŸŒ Converting to Meters (EPSG:6677)...\")\n",
        "    gdf_bound = gdf_bound.to_crs(\"EPSG:6677\")\n",
        "    gdf_ref = gdf_ref.to_crs(\"EPSG:6677\")\n",
        "\n",
        "    # --- å®Ÿé¨“ãƒ«ãƒ¼ãƒ— ---\n",
        "    buffer_sizes = [0.0, 5.0, 10.0]\n",
        "    total_points = len(gdf_ref)\n",
        "\n",
        "    print(f\"   Target Points: {total_points}\")\n",
        "    print(\"   --------------------------------------\")\n",
        "    print(\"   | Buffer | Accuracy  | Errors |\")\n",
        "    print(\"   --------------------------------------\")\n",
        "\n",
        "    for buf in buffer_sizes:\n",
        "        gdf_test = gdf_bound.copy()\n",
        "        if buf > 0: gdf_test['geometry'] = gdf_test.geometry.buffer(buf)\n",
        "\n",
        "        joined = gpd.sjoin(gdf_ref, gdf_test, how=\"left\", predicate=\"within\")\n",
        "        town_col = 'å¤§å­—_ä¸ç›®å' if 'å¤§å­—_ä¸ç›®å' in joined.columns else 'ä½å±…è¡¨ç¤ºä½æ‰€'\n",
        "        joined['is_match'] = joined.apply(lambda r: r['S_NAME'] in str(r[town_col]) if pd.notnull(r['S_NAME']) else False, axis=1)\n",
        "\n",
        "        # é‡è¤‡æŽ’é™¤å¾Œã®æ­£ç­”çŽ‡\n",
        "        point_results = joined.groupby(joined.index)['is_match'].any()\n",
        "        acc = point_results.mean() * 100\n",
        "        error_count = total_points - point_results.sum()\n",
        "\n",
        "        print(f\"   | {buf:4.1f}m  | {acc:.4f}%  | {error_count:4d}   |\")\n",
        "    print(\"   --------------------------------------\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_optimizer_metric()"
      ],
      "metadata": {
        "id": "YLbhKonQBjMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. è§£æ±ºç­–ï¼šãƒœãƒ­ãƒŽã‚¤åˆ†å‰² / æœ€è¿‘å‚åˆ¤å®š (The Voronoi Logic)\n",
        "\n",
        "ãƒãƒƒãƒ•ã‚¡æ³•ï¼ˆå£ã®æ‹¡å¤§ï¼‰ã§ã¯ã€ã€Œåºƒã„é“è·¯ï¼ˆå¹…å“¡20mè¶…ï¼‰ã€ã¨ã€Œéš£æŽ¥ã™ã‚‹ç‹­ã„è·¯åœ°ã€ã®ä¸¡ç«‹ãŒå›°é›£ã§ã‚ã‚‹ã“ã¨ãŒåˆ¤æ˜Žã—ã¾ã—ãŸã€‚\n",
        "ãã“ã§ã€å¹¾ä½•å­¦çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’**ã€ŒåŒ…å«åˆ¤å®šï¼ˆPoint in Polygonï¼‰ã€ã‹ã‚‰ã€Œæœ€è¿‘å‚åˆ¤å®šï¼ˆNearest Neighborï¼‰ã€ã¸è»¢æ›**ã—ã¾ã™ã€‚\n",
        "\n",
        "### ãƒ­ã‚¸ãƒƒã‚¯ã®æ¦‚è¦\n",
        "* **åŽŸç†:** å…¨ã¦ã®ç‚¹ã«ã¤ã„ã¦ã€ç‰©ç†çš„ã«**ã€Œæœ€ã‚‚è·é›¢ãŒè¿‘ã„å£ï¼ˆãƒãƒªã‚´ãƒ³ï¼‰ã€**ã‚’æŽ¢ã—å‡ºã—ã€ãã®ä½æ‰€ã‚’æŽ¡ç”¨ã—ã¾ã™ã€‚\n",
        "* **åŠ¹æžœ:** ã“ã‚Œã«ã‚ˆã‚Šã€é“è·¯ã®å¹…ãŒä½•ãƒ¡ãƒ¼ãƒˆãƒ«ã‚ã‚ã†ã¨ã‚‚ã€æ•°å­¦çš„ã«**ã€Œé“è·¯ã®ä¸­å¿ƒç·šã€**ã§å¢ƒç•Œã‚’å¼•ã„ãŸã®ã¨å…¨ãåŒã˜çµæžœãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚\n",
        "* **å®Ÿè£…:** åº§æ¨™ç³»ã‚’ãƒ¡ãƒ¼ãƒˆãƒ«å˜ä½ï¼ˆEPSG:6677ï¼‰ã«å¤‰æ›ã—ãŸä¸Šã§ã€`sjoin_nearest` ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é©ç”¨ã—ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "6Xs3gq7GB1a2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === CELL 5: Nearest Neighbor \"Voronoi\" Logic (Final Solution) ===\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import zipfile\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "BASE_PATH = \"/content/drive/MyDrive/0_Professional/TCE_Project/data\"\n",
        "\n",
        "def run_nearest_neighbor():\n",
        "    print(\"ðŸ’Ž Starting Nearest Neighbor (Voronoi) Processing...\")\n",
        "    if not os.path.exists(BASE_PATH): return\n",
        "\n",
        "    # --- ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹å®š ---\n",
        "    files = os.listdir(BASE_PATH)\n",
        "    shp_info = ref_info = None\n",
        "    for f in files:\n",
        "        if not f.lower().endswith(\".zip\"): continue\n",
        "        full_path = os.path.join(BASE_PATH, f)\n",
        "        try:\n",
        "            with zipfile.ZipFile(full_path, 'r') as z:\n",
        "                has_shp = any(n.lower().endswith('.shp') for n in z.namelist())\n",
        "                csv_files = [n for n in z.namelist() if n.lower().endswith('.csv') and not n.startswith('__')]\n",
        "                if has_shp and (shp_info is None) and (\"13101\" in f or \"r2ka\" in f or \"A002\" in f): shp_info = full_path\n",
        "                if len(csv_files) > 0 and (ref_info is None): ref_info = (full_path, csv_files[0])\n",
        "        except: continue\n",
        "\n",
        "    if not shp_info or not ref_info: return\n",
        "    print(f\"   SHP: {os.path.basename(shp_info)}\")\n",
        "\n",
        "    # --- ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ & æŠ•å½±å¤‰æ› ---\n",
        "    gdf_bound = gpd.read_file(f\"zip://{shp_info}\", encoding='cp932', engine='pyogrio')\n",
        "    gdf_bound = gdf_bound[['KEY_CODE', 'S_NAME', 'geometry']]\n",
        "    with zipfile.ZipFile(ref_info[0], 'r') as z:\n",
        "        with z.open(ref_info[1]) as f: df_ref = pd.read_csv(f, encoding='cp932')\n",
        "    gdf_ref = gpd.GeoDataFrame(df_ref, geometry=gpd.points_from_xy(df_ref['çµŒåº¦'], df_ref['ç·¯åº¦']), crs=\"EPSG:6668\")\n",
        "\n",
        "    print(\"ðŸŒ Converting to Metric System (EPSG:6677)...\")\n",
        "    gdf_bound = gdf_bound.to_crs(\"EPSG:6677\")\n",
        "    gdf_ref = gdf_ref.to_crs(\"EPSG:6677\")\n",
        "\n",
        "    # --- ã‚³ã‚¢å‡¦ç†: æœ€è¿‘å‚çµåˆ ---\n",
        "    print(\"âš¡ Calculating Nearest Polygons...\")\n",
        "    joined = gpd.sjoin_nearest(gdf_ref, gdf_bound, how=\"left\", distance_col=\"dist_m\")\n",
        "\n",
        "    # --- è©•ä¾¡ ---\n",
        "    town_col = 'å¤§å­—_ä¸ç›®å' if 'å¤§å­—_ä¸ç›®å' in joined.columns else 'ä½å±…è¡¨ç¤ºä½æ‰€'\n",
        "    joined['is_match'] = joined.apply(lambda r: r['S_NAME'] in str(r[town_col]), axis=1)\n",
        "\n",
        "    total = len(joined)\n",
        "    correct = joined['is_match'].sum()\n",
        "    error_count = total - correct\n",
        "    accuracy = (correct / total) * 100\n",
        "\n",
        "    print(f\"\\n=== ðŸ† Final Result ===\")\n",
        "    print(f\"   Accuracy:       {accuracy:.4f}%\")\n",
        "    print(f\"   Errors:         {error_count}\")\n",
        "\n",
        "    # æ•‘æ¸ˆã•ã‚ŒãŸç‚¹ï¼ˆå£ã‹ã‚‰é›¢ã‚Œã¦ã„ãŸãŒæ­£è§£ã—ãŸç‚¹ï¼‰\n",
        "    rescued = joined[(joined['dist_m'] > 0) & (joined['is_match'])]\n",
        "    if len(rescued) > 0:\n",
        "        print(f\"\\nâœ¨ Rescued Points (Road/Gap logic works): {len(rescued)}\")\n",
        "        print(rescued[[town_col, 'S_NAME', 'dist_m']].sort_values('dist_m', ascending=False).head(3))\n",
        "\n",
        "    # æ®‹ã£ãŸã‚¨ãƒ©ãƒ¼\n",
        "    if error_count > 0:\n",
        "        print(f\"\\nâš ï¸ Remaining Errors (Border Jitter): {error_count}\")\n",
        "        print(joined[~joined['is_match']][[town_col, 'S_NAME', 'dist_m']].head(3))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_nearest_neighbor()"
      ],
      "metadata": {
        "id": "7WnnJeBsCABP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. æœ€çµ‚ç›£æŸ»ï¼šæ®‹å­˜ã‚¨ãƒ©ãƒ¼ã®å¯è¦–åŒ– (Final Audit)\n",
        "\n",
        "Voronoiãƒ­ã‚¸ãƒƒã‚¯ã«ã‚ˆã‚Šã€é“è·¯ä¸Šã®ã€Œé›£æ°‘ã€ã¯æ•‘æ¸ˆã•ã‚Œã€ç²¾åº¦ã¯ **99.43%** ã«é”ã—ã¾ã—ãŸã€‚\n",
        "æœ€å¾Œã«ã€ãã‚Œã§ã‚‚æ®‹ã£ãŸ **ç´„0.6%ï¼ˆ10ä»¶å‰å¾Œï¼‰ã®ã‚¨ãƒ©ãƒ¼** ã®æ­£ä½“ã‚’çªãæ­¢ã‚ã¾ã™ã€‚\n",
        "\n",
        "### ç›£æŸ»ã®ç›®çš„\n",
        "ã“ã‚Œã‚‰ã®ã‚¨ãƒ©ãƒ¼ãŒã€Œãƒ­ã‚¸ãƒƒã‚¯ã®æ¬ é™¥ã€ãªã®ã‹ã€ãã‚Œã¨ã‚‚ã€Œãƒ‡ãƒ¼ã‚¿ã®é™ç•Œï¼ˆBorder Jitterï¼‰ã€ãªã®ã‹ã‚’ç¢ºå®šã•ã›ã¾ã™ã€‚\n",
        "åœ°å›³ä¸Šã§ç¢ºèªã—ã€ã‚¨ãƒ©ãƒ¼ç­‰ã®ç‚¹ãŒ**ã€Œå¢ƒç•Œç·šã‚®ãƒªã‚®ãƒªï¼ˆæ•°ãƒ¡ãƒ¼ãƒˆãƒ«ä»¥å†…ï¼‰ã€**ã«å­˜åœ¨ã—ã¦ã„ã‚Œã°ã€ã“ã‚Œã¯å»ºç‰©ã®å…¥ã‚Šå£ä½ç½®ã¨é‡å¿ƒä½ç½®ã®ã‚ºãƒ¬ã‚„æ¸¬é‡èª¤å·®ã«èµ·å› ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€æœ¬ã‚¨ãƒ³ã‚¸ãƒ³ã®**ç†è«–çš„é™ç•Œå€¤ï¼ˆTheoretical Limitï¼‰**ã«åˆ°é”ã—ãŸã¨çµè«–ä»˜ã‘ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "0-TIL7o-CCvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === CELL 6: Visualizing Final Errors (Border Jitter) ===\n",
        "import os\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import folium\n",
        "import zipfile\n",
        "import warnings\n",
        "from google.colab import drive\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "BASE_PATH = \"/content/drive/MyDrive/0_Professional/TCE_Project/data\"\n",
        "\n",
        "def visualize_final_errors():\n",
        "    if not os.path.exists(BASE_PATH): return\n",
        "\n",
        "    # ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹å®šã¨ãƒ­ãƒ¼ãƒ‰ (CELL 5ã¨åŒæ§˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç°¡ç•¥åŒ–)\n",
        "    files = os.listdir(BASE_PATH)\n",
        "    shp_info = ref_info = None\n",
        "    for f in files:\n",
        "        if not f.lower().endswith(\".zip\"): continue\n",
        "        full_path = os.path.join(BASE_PATH, f)\n",
        "        try:\n",
        "            with zipfile.ZipFile(full_path, 'r') as z:\n",
        "                has_shp = any(n.lower().endswith('.shp') for n in z.namelist())\n",
        "                csv_files = [n for n in z.namelist() if n.lower().endswith('.csv') and not n.startswith('__')]\n",
        "                if has_shp and (shp_info is None) and (\"13101\" in f or \"r2ka\" in f or \"A002\" in f): shp_info = full_path\n",
        "                if len(csv_files) > 0 and (ref_info is None): ref_info = (full_path, csv_files[0])\n",
        "        except: continue\n",
        "\n",
        "    if not shp_info or not ref_info: return\n",
        "\n",
        "    # Load & Project\n",
        "    gdf_bound = gpd.read_file(f\"zip://{shp_info}\", encoding='cp932', engine='pyogrio')\n",
        "    gdf_bound = gdf_bound[['KEY_CODE', 'S_NAME', 'geometry']]\n",
        "    with zipfile.ZipFile(ref_info[0], 'r') as z:\n",
        "        with z.open(ref_info[1]) as f: df_ref = pd.read_csv(f, encoding='cp932')\n",
        "    gdf_ref = gpd.GeoDataFrame(df_ref, geometry=gpd.points_from_xy(df_ref['çµŒåº¦'], df_ref['ç·¯åº¦']), crs=\"EPSG:6668\")\n",
        "\n",
        "    gdf_bound_m = gdf_bound.to_crs(\"EPSG:6677\")\n",
        "    gdf_ref_m = gdf_ref.to_crs(\"EPSG:6677\")\n",
        "\n",
        "    # Nearest Join\n",
        "    joined = gpd.sjoin_nearest(gdf_ref_m, gdf_bound_m, how=\"left\", distance_col=\"dist_m\")\n",
        "\n",
        "    # Identify Errors\n",
        "    town_col = 'å¤§å­—_ä¸ç›®å' if 'å¤§å­—_ä¸ç›®å' in joined.columns else 'ä½å±…è¡¨ç¤ºä½æ‰€'\n",
        "    joined['is_match'] = joined.apply(lambda r: r['S_NAME'] in str(r[town_col]), axis=1)\n",
        "    errors = joined[~joined['is_match']].copy()\n",
        "\n",
        "    print(f\"ðŸ” Plotting {len(errors)} error points...\")\n",
        "\n",
        "    # Map (WGS84)\n",
        "    gdf_bound_wgs = gdf_bound.to_crs(epsg=4326)\n",
        "    errors_wgs = errors.to_crs(epsg=4326)\n",
        "\n",
        "    if len(errors_wgs) > 0:\n",
        "        center = errors_wgs.geometry.iloc[0]\n",
        "        m = folium.Map(location=[center.y, center.x], zoom_start=16, tiles='OpenStreetMap')\n",
        "\n",
        "        folium.GeoJson(\n",
        "            gdf_bound_wgs,\n",
        "            style_function=lambda x: {'color': 'blue', 'weight': 1, 'fillOpacity': 0.05},\n",
        "            tooltip=folium.GeoJsonTooltip(fields=['S_NAME'])\n",
        "        ).add_to(m)\n",
        "\n",
        "        for idx, row in errors_wgs.iterrows():\n",
        "            folium.Marker(\n",
        "                location=[row.geometry.y, row.geometry.x],\n",
        "                popup=f\"<b>CONFLICT</b><br>True: {row[town_col]}<br>TCE: {row['S_NAME']}<br>Dist: {row['dist_m']:.2f}m\",\n",
        "                icon=folium.Icon(color='red', icon='exclamation', prefix='fa')\n",
        "            ).add_to(m)\n",
        "\n",
        "        display(m)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    visualize_final_errors()"
      ],
      "metadata": {
        "id": "LCL5yl32CDJ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}